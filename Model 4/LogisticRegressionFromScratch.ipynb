{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: ML Life Cycle: Modeling\n",
    " ## Building a Logistic Regression Model From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will continue working with the modeling phase of the machine learning life cycle. You will take what you have learned about gradient descent and write a Python class from scratch to train a logistic regression model. You will implement the various mathematical functions learned in the course, such as the gradient and Hessian of the log loss. \n",
    "\n",
    "In the course videos, we presented functions that compute the log loss, gradient, and Hessian and that implement gradient descent for logistic regression. You will do similar work here, only we'll refactor the code to improve its generality. \n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "1. Build a class that can:\n",
    "    * Fit a logistic regression model given training data \n",
    "    * Make predictions\n",
    "2. Build your DataFrame and define your ML problem:\n",
    "    * Load the Airbnb \"listings\" data set into a DataFrame\n",
    "    * Define the label - what are you predicting?\n",
    "    * Identify features\n",
    "3. Create labeled examples from the data set\n",
    "5. Train a logistic regression classifier using your class\n",
    "6. Benchmark our class against scikit-learn's logistic regression class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Logistic Regression Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below contains the logistic regression class that we are building. Your task is to complete the logic within each specified method. Remember, a method is just a function that belongs to that particular class.\n",
    "\n",
    "Below is a breakdown of the methods contained in the class:\n",
    "\n",
    "1. An `__init__()` method that takes in an error tolerance as a stopping criterion, as well as max number of iterations.\n",
    "2. A `predict_proba()` method that takes a given matrix of features $X$ and predicts $P = \\dfrac{1}{1+e^{-(X \\cdot W+\\alpha)}}$ for each entry\n",
    "3. A `compute_gradient()` method that computes the gradient vector $G$\n",
    "4. A `compute_hessian()` method that computes the Hessian. Note that the $H$ can be broken down to the following matrix multiplications: $H=(X^T*Q)\\cdot X$. \n",
    "5. An `update_weights()` method that applies gradient descent to update the weights\n",
    "6. A `check_stop()` method that checks whether the model has converged or the max iterations have been met\n",
    "7. A `fit()` method that trains the model. It takes in the data and runs the gradient optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Complete the Class\n",
    "\n",
    "<b>Task</b>: Follow the steps below to complete the code in the `LogisticRegressionScratch` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Step A\n",
    "\n",
    "Complete the `self.predict_proba()` method. (<b>Note</b>: This implementation looks a little bit different from the formula you have seen previously. This is simply because we will absorb the intercept term into our `X` matrix). Do the following: \n",
    "1. Create a variable `XW`. Assign it the result of the dot product of the input `X` and `self.weights_array` variable\n",
    "2. Create a variable `P`. Assign it the result of the inverse logit $(1+e^{-XW})^{-1}$\n",
    "3. Make sure the method returns the variable `P` (the `return` statement has been provided for you).\n",
    "\n",
    "\n",
    "#### Step B\n",
    "\n",
    "Complete the `self.compute_gradient()` method. This is where we implement the log loss gradient. Do the following:\n",
    "1. Create a variable `G`. Assign it the result of the gradient computation $-(y-P) \\cdot X$\n",
    "2. Make sure the method returns the variable `G` (the `return` statement has been provided for you).\n",
    "\n",
    "\n",
    "#### Step C\n",
    "\n",
    "Complete the `self.compute_hessian()` method. This is where we implement the log loss Hessian. Do the following:\n",
    "1. Create a variable `Q`. Assign it the result of the following computation $P*(1-P)$\n",
    "2. Create a variable `XQ`. Assign it the result of the following computation $X^T * Q$. Note that $X$ is the input to the method and this is using regular multiplication\n",
    "3. Create a variable called `H`. Assign it the result of the following computation $XQ \\cdot X$. Note that this operation is using the dot product for matrix multiplication\n",
    "4. Make sure the method returns the variable `H` (the `return` statement has been provided for you).\n",
    "\n",
    "\n",
    "#### Step D\n",
    "\n",
    "Complete the `self.update_weights()` method. This is where we implement the gradient descent update. Do the following:\n",
    "1. Create a variable `P`. Call the `self.predict_proba()` method to get predictions and assign the result to variable `P`. Note, when calling a method from within the class you need to call it using `self.predict_proba()`.\n",
    "2. Create a variable `G`. Call the `self.compute_gradient()` method and assign the result to variable `G`.\n",
    "3. Create a variable `H`. Call the `self.compute_hessian()` method to get the Hessian and assign the result to variable `H`.\n",
    "4. Assign the `self.weights_array` variable  to the `self.prior_w` variable. By doing so, the current weight values become the previous weight values.\n",
    "5. Compute the gradient update-step, which is governed by $w_t=w_{t-1}-H^{-1} \\cdot G$, where $w_t$ and $w_{t-1}$ are both the variable `self.weights_array`(You are updating the current weights and therefore want to update the values in `self.weights_array`).  *Hint*: to implement the part $H^{-1} \\cdot G$, use NumPy's `np.linalg.inv()` function and `dot()` method.\n",
    "6. Note: this method does not return any value.\n",
    "\n",
    "\n",
    "#### Step E\n",
    "\n",
    "Complete the `self.check_stop()` method. This is where we implement the stopping criteria. Do the following:\n",
    "1. Create a variable called `w_old_norm`. Normalize `self.prior_w`. You normalize a vector `v` using the following formula $v / \\|v\\|$ where $\\|v\\|$ can be computed using the function `np.linalg.norm(v)`. Assign this result to the variable `w_old_norm`.\n",
    "2. Create a variable called `w_new_norm`. Normalize `self.weights_array` following the same approach. Assign the result to the variable `w_new_norm`. \n",
    "3. Create a variable called `diff` and assign it the value `w_old_norm-w_new_norm`.\n",
    "4. Create a variable called `distance`. Compute $\\sqrt{d \\cdot d}$ where $d$ is the variable `diff` created in the step above. Note that this uses the dot product.\n",
    "5. Create a boolean variable called `stop`. Check whether `distance` is less than `self.tolerance`. If so, assign `True` to the variable `stop`. If not, assign `False` to the variable `stop`.\n",
    "6. Make sure the method returns the variable `stop` (the `return` statement has been provided for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionScratch(object):\n",
    "    \n",
    "    def __init__(self, tolerance = 10**-8, max_iterations = 20):\n",
    "        \n",
    "        self.tolerance = tolerance\n",
    "        self.max_iterations = max_iterations\n",
    "        self.weights_array = None # holds current weights and intercept (intercept is at the last position)\n",
    "        self.prior_w = None # holds previous weights and intercept (intercept is at the last position)\n",
    "        \n",
    "        # once we are done training, these variables will hold the \n",
    "        # final values for the weights and intercept\n",
    "        self.weights = None\n",
    "        self.intercept = None \n",
    "\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Compute probabilities using the inverse logit\n",
    "        - Inputs: The Nx(K+1) matrix with intercept column X\n",
    "        - Outputs: Vector of probabilities of length N\n",
    "        '''\n",
    "        \n",
    "        ### STEP A - WRITE YOUR CODE HERE\n",
    "        XW = np.dot(X, self.weights_array)\n",
    "        P =  1 / (1 + np.exp(-XW))\n",
    "        return P\n",
    "\n",
    "    \n",
    "    \n",
    "    def compute_gradient(self, X, Y, P):\n",
    "        '''\n",
    "        Computes the gradient vector\n",
    "        -Inputs:\n",
    "            - The Nx(K+1) matrix with intercept column X\n",
    "            - Nx1 vector y (label) \n",
    "            - Nx1 vector of predictions P\n",
    "        -Outputs: 1x(K+1) vector of gradients\n",
    "        '''\n",
    "        \n",
    "        ### STEP B - WRITE YOUR CODE HERE\n",
    "        G = -np.dot((Y-P), X)  \n",
    "        return G\n",
    "        \n",
    "    def compute_hessian(self, X, P):\n",
    "        '''\n",
    "        computes the Hessian matrix\n",
    "        -inputs:\n",
    "            - Nx(K+1) matrix X\n",
    "            - Nx1 vector of predictions P\n",
    "        -outputs:\n",
    "            - KxK Hessian matrix H=X^T * Diag(Q) * X\n",
    "        '''\n",
    "\n",
    "        ### STEP C - WRITE YOUR CODE HERE\n",
    "        Q = P * (1-P)\n",
    "        XQ = X.T * Q\n",
    "        \n",
    "        H = np.dot(XQ, X)\n",
    "        return H\n",
    "\n",
    "\n",
    "    def update_weights(self, X, y):\n",
    "        '''\n",
    "        Updates existing weight vector\n",
    "        -Inputs:\n",
    "            -Nx(Kx1) matrix X\n",
    "            -Nx1 vector y\n",
    "        -Calls predict_proba, compute_gradient and compute_hessian and uses the \n",
    "        return values to update the weights array\n",
    "        '''\n",
    "        \n",
    "        ### STEP D - WRITE YOUR CODE HERE\n",
    "        P = self.predict_proba(X)\n",
    "        G = self.compute_gradient(X, y, P)\n",
    "        H = self.compute_hessian(X, P)\n",
    "\n",
    "        self.prior_w = self.weights_array.copy()\n",
    "        self.weights_array -= np.dot(np.linalg.inv(H), G)        \n",
    "        \n",
    "      \n",
    "           \n",
    "    def check_stop(self):\n",
    "        '''\n",
    "        check to see if euclidean distance between old and new weights (normalized)\n",
    "        is less than the tolerance\n",
    "        \n",
    "        returns: True or False on whether stopping criteria is met\n",
    "        '''\n",
    "        \n",
    "        ### STEP E - WRITE YOUR CODE HERE\n",
    "        w_old_norm = self.prior_w / np.linalg.norm(self.prior_w)\n",
    "        w_new_norm = self.weights_array / np.linalg.norm(self.weights_array)\n",
    "        diff = w_old_norm - w_new_norm\n",
    "        distance = np.sqrt(np.dot(diff, diff))\n",
    "        \n",
    "        stop = True\n",
    "        if distance < self.tolerance:\n",
    "            stop = True\n",
    "        else:\n",
    "            stop = False\n",
    "        \n",
    "        return stop\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        X is the Nx(K-1) data matrix\n",
    "        Y is the labels, using {0,1} coding\n",
    "        '''\n",
    "        \n",
    "        #set initial weights - add an extra dimension for the intercept\n",
    "        self.weights_array = np.zeros(X.shape[1] + 1)\n",
    "        \n",
    "        #Initialize the slope parameter to log(base rate/(1-base rate))\n",
    "        self.weights_array[-1] = np.log(y.mean() / (1-y.mean()))\n",
    "        \n",
    "        #create a new X matrix that includes a column of ones for the intercept\n",
    "        X_int = np.hstack((X, np.ones((X.shape[0],1))))\n",
    "\n",
    "        for i in range(self.max_iterations):\n",
    "            self.update_weights(X_int, y)\n",
    "            \n",
    "            # check whether we should\n",
    "            stop = self.check_stop()\n",
    "            if stop:\n",
    "                # since we are stopping, lets save the final weights and intercept\n",
    "                self.set_final_weights()\n",
    "                self.set_final_intercept()\n",
    "                break\n",
    "                \n",
    "    \n",
    "    def set_final_weights(self):\n",
    "        self.weights = self.weights_array[0:-1]\n",
    "        \n",
    "    def set_final_intercept(self):\n",
    "        self.intercept = self.weights_array[-1]  \n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def get_intercept(self):\n",
    "        return self.intercept\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Use the Class to Train a Logistic Regression Model\n",
    "\n",
    "Now we will test our implementation of logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Build Your DataFrame and Define Your ML Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a Data Set and Save it as a Pandas DataFrame\n",
    "\n",
    "We will work with the data set ``airbnbData_train``. This data set already has all the necessary preprocessing steps implemented, including one-hot encoding of the categorical variables, scaling of all numerical variable values, and imputing missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.getcwd(), \"data\", \"airbnbData_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Load the data and save it to DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Label\n",
    "\n",
    "Your goal is to train a machine learning model that predicts whether an Airbnb host is a 'super host'. This is an example of supervised learning and is a binary classification problem. In our dataset, our label will be the `host_is_superhost` column and the label will either contain the value `True` or `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Features\n",
    "\n",
    "We have chosen to train the model on a subset of features that can help make with our predictive problem, that is, they can help predict with the host is a super host. Run the following cell to see the list of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['review_scores_rating',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_value',\n",
       " 'host_response_rate',\n",
       " 'host_acceptance_rate']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list = ['review_scores_rating','review_scores_cleanliness','review_scores_checkin','review_scores_communication','review_scores_value','host_response_rate','host_acceptance_rate']\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Create Labeled Examples from the Data Set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: Our data is ready for modeling. Obtain the feature columns from DataFrame `df` and assign to `X`. Obtain the label column from DataFrame `df` and assign to `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[feature_list]\n",
    "y = df['host_is_superhost']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Train a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our labeled examples, let's test out our logistic regression class. <b>Note:</b> We will not be splitting our data intro training and test data sets\n",
    "\n",
    "<b>Task:</b> In the code cell below, do the following:\n",
    "1. Create an instance of `LogisticRegressionScratch()` using default parameters (i.e. do not supply any arguments). Name this instance `lr`.\n",
    "2. Fit the model `lr` to the training data by calling `lr.fit()` with X and y as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24341589 0.24341589 0.24341589 ... 0.24341589 0.24341589 0.24341589]\n",
      "[-2.23444015e+03 -2.36061829e+03 -1.58706936e+03 -1.65428172e+03\n",
      " -1.81627382e+03 -2.45033748e+03 -2.07127996e+03 -8.28226376e-13]\n",
      "[[ 5.16066023e+03  3.91288107e+03  3.55131655e+03  3.75566339e+03\n",
      "   4.23499762e+03  4.74210519e+02  5.07592354e+01 -1.19992905e-12]\n",
      " [ 3.91288107e+03  5.16066023e+03  2.94846671e+03  2.92623244e+03\n",
      "   3.54781321e+03  4.71993900e+02  2.17995977e+02 -2.17248441e-12]\n",
      " [ 3.55131655e+03  2.94846671e+03  5.16066023e+03  3.90221121e+03\n",
      "   3.32537112e+03  4.69156594e+02  1.98417051e+01 -7.63922259e-12]\n",
      " [ 3.75566339e+03  2.92623244e+03  3.90221121e+03  5.16066023e+03\n",
      "   3.52873308e+03  4.66461400e+02  9.81017171e+00 -2.54518628e-12]\n",
      " [ 4.23499762e+03  3.54781321e+03  3.32537112e+03  3.52873308e+03\n",
      "   5.16066023e+03  3.49444148e+02  3.35020910e+01  3.60467212e-12]\n",
      " [ 4.74210519e+02  4.71993900e+02  4.69156594e+02  4.66461400e+02\n",
      "   3.49444148e+02  5.16066023e+03  2.23754610e+03  3.83515442e-12]\n",
      " [ 5.07592354e+01  2.17995977e+02  1.98417051e+01  9.81017171e+00\n",
      "   3.35020910e+01  2.23754610e+03  5.16066023e+03 -1.78701498e-12]\n",
      " [-1.25677246e-12 -2.07389661e-12 -7.37232497e-12 -2.40529818e-12\n",
      "   3.75477427e-12  3.77831100e-12 -1.73727699e-12  5.16066023e+03]]\n",
      "[ 0.24181003  0.27731681 -0.02031312  0.01022281 -0.05463678  0.32482408\n",
      "  0.24684366 -1.13404226]\n",
      "[0.12066144 0.05063089 0.22091317 ... 0.02244538 0.3570379  0.29497462]\n",
      "[-803.6818749  -768.12014663 -658.74739792 -674.90568218 -687.37831543\n",
      " -908.93650598 -481.81323605  528.48728338]\n",
      "[[ 2.32216979e+03  1.59608646e+03  1.35462265e+03  1.46249233e+03\n",
      "   1.82480716e+03  2.06695148e+02  4.80154985e+01  8.41693951e+02]\n",
      " [ 1.59608646e+03  2.52321194e+03  1.03109108e+03  1.01341694e+03\n",
      "   1.42646013e+03  2.72005619e+02  2.18794407e+02  9.19859704e+02]\n",
      " [ 1.35462265e+03  1.03109108e+03  2.30935131e+03  1.50647122e+03\n",
      "   1.26841787e+03  1.61598500e+02  1.57676793e+01  5.64664281e+02]\n",
      " [ 1.46249233e+03  1.01341694e+03  1.50647122e+03  2.34648177e+03\n",
      "   1.39453725e+03  1.71771015e+02 -3.04342989e+00  5.94098409e+02]\n",
      " [ 1.82480716e+03  1.42646013e+03  1.26841787e+03  1.39453725e+03\n",
      "   2.69809769e+03  1.42083015e+02  4.19840450e+01  6.77943353e+02]\n",
      " [ 2.06695148e+02  2.72005619e+02  1.61598500e+02  1.71771015e+02\n",
      "   1.42083015e+02  1.92138807e+03  9.60206654e+02  9.27921087e+02]\n",
      " [ 4.80154985e+01  2.18794407e+02  1.57676793e+01 -3.04342989e+00\n",
      "   4.19840450e+01  9.60206654e+02  3.65232397e+03  8.54814637e+02]\n",
      " [ 8.41693951e+02  9.19859704e+02  5.64664281e+02  5.94098409e+02\n",
      "   6.77943353e+02  9.27921087e+02  8.54814637e+02  5.19466755e+03]]\n",
      "[ 0.4325292   0.43477479  0.05964167  0.08818065 -0.0479254   0.85342269\n",
      "  0.29327908 -1.41511075]\n",
      "[0.06219264 0.00290407 0.25832511 ... 0.00109826 0.37131827 0.29152558]\n",
      "[-247.06220585 -213.04026743 -234.83372417 -241.65775394 -224.33873121\n",
      " -374.74630788 -117.45820146  252.06068639]\n",
      "[[1489.41385651 1059.49936881  774.06927119  831.2483129  1129.0423502\n",
      "   278.68667809  120.5221982  1164.46945557]\n",
      " [1059.49936881 1741.53293939  611.1752846   604.71993296  917.80039363\n",
      "   348.59506342  266.03409018 1236.02748379]\n",
      " [ 774.06927119  611.1752846  1301.2554544   773.29128389  700.81254686\n",
      "   199.22845244   77.99522849  811.4487735 ]\n",
      " [ 831.2483129   604.71993296  773.29128389 1308.47789829  769.68266678\n",
      "   208.76382627   65.65093046  849.41559132]\n",
      " [1129.0423502   917.80039363  700.81254686  769.68266678 1838.74010236\n",
      "   202.98828836  105.29029312  950.28751619]\n",
      " [ 278.68667809  348.59506342  199.22845244  208.76382627  202.98828836\n",
      "  1002.7199708   685.27782201 1226.81198914]\n",
      " [ 120.5221982   266.03409018   77.99522849   65.65093046  105.29029312\n",
      "   685.27782201 3195.83653148  999.48339276]\n",
      " [1164.46945557 1236.02748379  811.4487735   849.41559132  950.28751619\n",
      "  1226.81198914  999.48339276 4801.74859006]]\n",
      "[ 0.53674958  0.48801811  0.15910957  0.19320735 -0.01666085  1.51115081\n",
      "  0.26750735 -1.71083998]\n",
      "[3.41208194e-02 9.37353681e-05 3.32122285e-01 ... 1.40955565e-04\n",
      " 3.45083732e-01 2.67045389e-01]\n",
      "[-36.51192394 -22.45490579 -43.67881874 -47.99877101 -35.67159711\n",
      " -59.74798587 -18.3295755  102.87869758]\n",
      "[[1244.40000563  924.97280637  632.19159277  672.01619933  933.85072566\n",
      "   323.08456179  136.97654807 1240.39047417]\n",
      " [ 924.97280637 1505.25961499  529.47540662  529.22921647  796.01868161\n",
      "   385.05648046  268.52897659 1292.89808118]\n",
      " [ 632.19159277  529.47540662  974.98228217  579.39168776  562.0380553\n",
      "   236.77189715   96.20283441  889.82609257]\n",
      " [ 672.01619933  529.22921647  579.39168776  956.44849322  605.37541694\n",
      "   245.52467176   88.71194358  930.44775225]\n",
      " [ 933.85072566  796.01868161  562.0380553   605.37541694 1551.40065584\n",
      "   242.62130492  119.13634974 1019.7705007 ]\n",
      " [ 323.08456179  385.05648046  236.77189715  245.52467176  242.62130492\n",
      "   881.15132565  613.34506821 1326.13714376]\n",
      " [ 136.97654807  268.52897659   96.20283441   88.71194358  119.13634974\n",
      "   613.34506821 3145.13585626  968.72306604]\n",
      " [1240.39047417 1292.89808118  889.82609257  930.44775225 1019.7705007\n",
      "  1326.13714376  968.72306604 4492.57341611]]\n",
      "[ 0.5654055   0.49231701  0.19796551  0.24854873 -0.00646213  1.70619026\n",
      "  0.26457174 -1.82130099]\n",
      "[2.74171553e-02 3.30867241e-05 3.49332241e-01 ... 7.62311391e-05\n",
      " 3.34669491e-01 2.56663609e-01]\n",
      "[-2.37760211 -0.92997914 -3.49718625 -4.30879668 -2.45226417 -1.48058352\n",
      " -0.37895087  8.4751942 ]\n",
      "[[1182.296109    890.99383401  599.49647519  634.58609378  885.93068908\n",
      "   333.41249484  142.92607607 1247.09005305]\n",
      " [ 890.99383401 1444.89977076  510.5374836   511.79094886  765.9671464\n",
      "   391.52588348  269.78031574 1290.70722841]\n",
      " [ 599.49647519  510.5374836   892.26105724  534.91001755  529.93502237\n",
      "   246.8611115   102.65049804  904.07850407]\n",
      " [ 634.58609378  511.79094886  534.91001755  863.30146409  565.8870456\n",
      "   256.44690824   97.22280124  947.10949823]\n",
      " [ 885.93068908  765.9671464   529.93502237  565.8870456  1475.66267979\n",
      "   252.31289763  124.19780187 1027.16229216]\n",
      " [ 333.41249484  391.52588348  246.8611115   256.44690824  252.31289763\n",
      "   865.20324861  598.5379317  1336.2905801 ]\n",
      " [ 142.92607607  269.78031574  102.65049804   97.22280124  124.19780187\n",
      "   598.5379317  3106.30712095  958.72052172]\n",
      " [1247.09005305 1290.70722841  904.07850407  947.10949823 1027.16229216\n",
      "  1336.2905801   958.72052172 4376.53041855]]\n",
      "[ 0.56690026  0.49225756  0.20156346  0.25544724 -0.00590701  1.71589801\n",
      "  0.26478598 -1.82902331]\n",
      "[2.70233884e-02 3.13352966e-05 3.50015771e-01 ... 7.29314762e-05\n",
      " 3.34254761e-01 2.56165171e-01]\n",
      "[-0.01801946 -0.00664105 -0.02899363 -0.04114131 -0.01915636  0.00086749\n",
      "  0.00307687  0.04219097]\n",
      "[[1177.98339109  888.73891955  597.10113097  631.7509908   882.61398707\n",
      "   334.16913676  143.62503546 1248.00200932]\n",
      " [ 888.73891955 1440.96529279  509.21431947  510.57643393  763.96771994\n",
      "   391.93103692  270.03480548 1290.68313376]\n",
      " [ 597.10113097  509.21431947  885.74091973  531.41801862  527.5434496\n",
      "   247.68593785  103.37735897  905.83031232]\n",
      " [ 631.7509908   510.57643393  531.41801862  855.24958335  562.77898963\n",
      "   257.43637252   98.17438973  949.40393434]\n",
      " [ 882.61398707  763.96771994  527.5434496   562.77898963 1470.36963612\n",
      "   253.04466475  124.80887046 1028.14083321]\n",
      " [ 334.16913676  391.93103692  247.68593785  257.43637252  253.04466475\n",
      "   864.08030241  597.65760426 1336.09296796]\n",
      " [ 143.62503546  270.03480548  103.37735897   98.17438973  124.80887046\n",
      "   597.65760426 3102.04744327  958.06621855]\n",
      " [1248.00200932 1290.68313376  905.83031232  949.40393434 1028.14083321\n",
      "  1336.09296796  958.06621855 4368.47108971]]\n",
      "[ 0.56690006  0.492255    0.201587    0.25551467 -0.00590516  1.71592957\n",
      "  0.26478817 -1.82906226]\n",
      "[2.70215510e-02 3.13293376e-05 3.50018295e-01 ... 7.29176892e-05\n",
      " 3.34254869e-01 2.56164490e-01]\n",
      "[-9.11931581e-07 -3.11269580e-07 -1.55473582e-06 -2.75985399e-06\n",
      " -1.04373386e-06  3.15266245e-07  3.15450752e-07  1.70860016e-06]\n",
      "[[1177.96031663  888.72781578  597.08629765  631.73219758  882.59607811\n",
      "   334.17360238  143.62996592 1248.01123645]\n",
      " [ 888.72781578 1440.94701879  509.20665449  510.56942255  763.95776379\n",
      "   391.93305153  270.03653929 1290.68486698]\n",
      " [ 597.08629765  509.20665449  885.69837982  531.39372299  527.5283059\n",
      "   247.69157227  103.38277007  905.84711142]\n",
      " [ 631.73219758  510.56942255  531.39372299  855.18694164  562.75723856\n",
      "   257.44410495   98.18177612  949.42873346]\n",
      " [ 882.59607811  763.95776379  527.5283059   562.75723856 1470.34093338\n",
      "   253.04916697  124.81326789 1028.15090617]\n",
      " [ 334.17360238  391.93305153  247.69157227  257.44410495  253.04916697\n",
      "   864.0738681   597.65276614 1336.08814057]\n",
      " [ 143.62996592  270.03653929  103.38277007   98.18177612  124.81326789\n",
      "   597.65276614 3102.02220813  958.06080268]\n",
      " [1248.01123645 1290.68486698  905.84711142  949.42873346 1028.15090617\n",
      "  1336.08814057  958.06080268 4368.43253545]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.56690005  0.492255    0.201587    0.25551467 -0.00590516  1.71592957\n",
      "  0.26478817 -1.82906226]\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegressionScratch()\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to see the resulting weights and intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fitted weights and intercept are:\n",
      "[ 0.56690005  0.492255    0.201587    0.25551467 -0.00590516  1.71592957\n",
      "  0.26478817] -1.829062262272181\n"
     ]
    }
   ],
   "source": [
    "print('The fitted weights and intercept are:')\n",
    "# print(lr.update_weights(X, y), lr.compute_hessian(), lr.compute_gradient(), lr.predict_proba())\n",
    "print(lr.get_weights(), lr.get_intercept())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Compare with Scikit-Learn's Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare our logistic regression implementation with the `sklearn` logistic regression implementation. Note that by default scikit-learn uses a different optimization technique. However, our goal is to compare our resulting weights and intercept with those of scikit-learn's implementation, and these should be the same.\n",
    " \n",
    "<b>Task:</b> In the code cell below, write code to does the following:\n",
    "1. Create the scikit-learn `LogisticRegression` model object below and assign to variable `lr_sk`. Use `C=10**10` as the argument to `LogisticRegression()`.\n",
    "\n",
    "2. Fit the model `lr_sk` to the training data by calling `lr_sk.fit()` with X and y as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000000000, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_sk = LogisticRegression(C=10**10)\n",
    "lr_sk.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to see the resulting weights and intercept. Compare these to our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fitted weights and intercept with sklearn are:\n",
      "[[ 0.56691547  0.49224905  0.20150113  0.25563246 -0.005929    1.71592022\n",
      "   0.26479199]] [-1.82906726]\n"
     ]
    }
   ],
   "source": [
    "print('The fitted weights and intercept with sklearn are:')\n",
    "print(lr_sk.coef_, lr_sk.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check the efficiency (or run time) of both methods. We will use the magic function `%timeit` to do this\n",
    "\n",
    "<b>Task:</b> Use the `%timeit` magic function to fit the logistic regression model `lr` on the training data. Hint: use `%timeit` on `lr.fit(X, y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.09 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "622 ms ± 303 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Use the `%timeit` magic function to fit the logistic regression model `lr_sk` on the training data. Take a look and see which one is faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.06 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "309 ms ± 119 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit lr_sk.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
